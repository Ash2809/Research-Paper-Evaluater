{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from typing import Literal, List\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "from langchain_core.prompts.chat import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = \"AIzaSyAEqCEpjV4_6nZFKGQv8cxiyffiUNdDjGE\"\n",
    "llm = ChatGoogleGenerativeAI(api_key = GOOGLE_API_KEY, model = \"gemini-1.5-flash\",temperature = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello there! How can I help you today?'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    result : str  \n",
    "    doc : str  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_extractor(state):\n",
    "    doc = state[\"doc\"]\n",
    "    loader = DirectoryLoader(r\"C:\\Projects\\Research-Paper-Evaluater\\data\\\\\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    all_text = \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "    return {\"doc\" : all_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_decider(state):\n",
    "    doc = state[\"doc\"]\n",
    "    print(doc[:100]) \n",
    "    \n",
    "    class decide(BaseModel):\n",
    "        Binary_Score: str = Field(..., description=\"Is this Report Publishable?, respond with Yes or No.\")\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(decide)\n",
    "    \n",
    "    system = \"\"\"\n",
    "    You are an AI system tasked with determining whether a given document is a publishable research paper. A publishable research paper must meet the following criteria:\n",
    "\n",
    "    1. Structure:\n",
    "    - The document includes a title, abstract, introduction, methodology, results, discussion, and references.\n",
    "    \n",
    "    2. **Content**:\n",
    "    - The document presents original research, analysis, or findings.\n",
    "    - It has a clear research question, hypothesis, or objective.\n",
    "    - The methodology is well-detailed and appropriate for the research question.\n",
    "    - Results are presented with supporting data, graphs, or tables.\n",
    "\n",
    "    3. **Language and Formatting**:\n",
    "    - The writing is clear, concise, and follows academic standards.\n",
    "    - Proper citations and references are included in a recognized citation style.\n",
    "\n",
    "    4. **Credibility**:\n",
    "    - Sources cited in the references are credible and relevant to the topic.\n",
    "    - The claims made are supported by sufficient evidence.\n",
    "\n",
    "    Your task is to analyze the provided document and respond with \"Yes\" if it meets these criteria for publication or \"No\" if it does not.\n",
    "\n",
    "    Document: {doc}\n",
    "    \"\"\"  \n",
    "    \n",
    "    binary_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", f\"paper: {doc}\")\n",
    "        ]\n",
    "    )\n",
    "        # Check if structured_llm is correctly set up\n",
    "    print(\"Calling LLM with the prompt...\")\n",
    "    llm\n",
    "    \n",
    "    grader_chain = binary_prompt | structured_llm\n",
    "    \n",
    "    try:\n",
    "        llm_response = grader_chain.invoke({\"doc\": doc})\n",
    "        print(\"LLM Response received successfully.\")\n",
    "        \n",
    "        prediction_output = llm_response.Binary_Score\n",
    "        print(prediction_output)\n",
    "        return {\"prediction\": prediction_output}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM invocation: {e}\")\n",
    "        return {\"prediction\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(state):\n",
    "     prediction = state[\"prediction\"]\n",
    "     doc = state[\"doc\"] \n",
    "\n",
    "     extract_prompt = \"\"\" \n",
    "    You are an AI system tasked with extracting true labels from the provided documents. \n",
    "    Each document contains a label indicating whether it is \"Yes\" (publishable) or \"No\" (not publishable). \n",
    "    Extract the true labels from each document and respond as a list of labels.\n",
    "    Documents: {doc}\n",
    "    \"\"\"\n",
    "     extraction_prompt=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", extract_prompt),\n",
    "            (\"human\", f\"paper: {doc}\")\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "     grader_chain= extraction_prompt | llm\n",
    "     extraction_response=grader_chain.invoke({doc})\n",
    "\n",
    "     if extraction_response and extraction_response.content.strip():\n",
    "        true_labels = extraction_response.content.strip()  # Strip extra spaces and newlines\n",
    "        true_labels = true_labels.replace(\"content=\", \"\").strip()  # Remove 'content=' part\n",
    "        true_labels = true_labels.replace(\"['\", \"\").replace(\"']\", \"\")  # Remove list brackets\n",
    "        true_labels = true_labels.strip()  # Remove leading/trailing spaces\n",
    "     else:\n",
    "        raise ValueError(\"Failed to extract true labels from the document.\")\n",
    "     print(true_labels)\n",
    "     calculation_prompt = \"\"\" \n",
    "    You are an AI system that calculates evaluation metrics for binary classification problems. \n",
    "    The F1-score is calculated using the formula:\n",
    "    \n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    Precision = True Positives / (True Positives + False Positives)\n",
    "    Recall = True Positives / (True Positives + False Negatives)\n",
    "    \n",
    "    Here is the data:\n",
    "    - True Labels: {true_labels}\n",
    "    - Predicted Labels: {prediction}\n",
    "    \n",
    "    Calculate the F1-score for the provided data and respond with the value.\n",
    "    \"\"\"\n",
    "     \n",
    "     calculating_prompt=ChatPromptTemplate.from_messages(\n",
    "          [\n",
    "               (\"system\",calculation_prompt)\n",
    "          ]\n",
    "     )\n",
    "     inputs_for_f1 = {\"true_labels\": true_labels, \"prediction\": prediction}\n",
    "     grader_chain=calculating_prompt|llm\n",
    "     calculation_response=grader_chain.invoke(inputs_for_f1)\n",
    "     print(f\"F1-Score: {calculation_response}\")\n",
    "    \n",
    "     return {\"f1_score\": calculation_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"PDF_Extractor\", pdf_extractor)\n",
    "workflow.add_node(\"Report_Generator\", report_decider)\n",
    "workflow.add_node(\"F1_calculator\",f1_score)\n",
    "\n",
    "#workflow.add_edge(START, \"PDF_Extractor\")\n",
    "workflow.add_edge(START, \"Report_Generator\")\n",
    "workflow.add_edge(\"Report_Generator\",\"F1_calculator\")\n",
    "workflow.add_edge(\"F1_calculator\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 200 characters of the document text:\n",
      "Examining the Convergence of Denoising Diffusion Probabilistic\n",
      "Models: A Quantitative Analysis\n",
      "Abstract\n",
      "Deep generative models, particularly diffusion models, are a significant family within deep lear\n",
      "Examining the Convergence of Denoising Diffusion Probabilistic\n",
      "Models: A Quantitative Analysis\n",
      "Abstr\n",
      "Calling LLM with the prompt...\n",
      "LLM Response received successfully.\n",
      "Yes\n",
      "final-ouput: {'prediction': 'Yes'}\n",
      "Report Generator Output: None\n",
      "Predictions: Yes\n",
      "Yes\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_google_genai\\chat_models.py:178\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgument\u001b[0m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions:\n\u001b[0;32m     31\u001b[0m     inputs_for_f1 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc\u001b[39m\u001b[38;5;124m\"\u001b[39m: sanitized_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: predictions}\n\u001b[1;32m---> 32\u001b[0m     f1_output\u001b[38;5;241m=\u001b[39m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_for_f1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f1_output)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[218], line 52\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     50\u001b[0m inputs_for_f1 \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: true_labels, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction}\n\u001b[0;32m     51\u001b[0m grader_chain\u001b[38;5;241m=\u001b[39mcalculating_prompt\u001b[38;5;241m|\u001b[39mllm\n\u001b[1;32m---> 52\u001b[0m calculation_response\u001b[38;5;241m=\u001b[39m\u001b[43mgrader_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_for_f1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcalculation_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculation_response}\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\runnables\\base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    287\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n\u001b[0;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    634\u001b[0m                 m,\n\u001b[0;32m    635\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    636\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    637\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    638\u001b[0m             )\n\u001b[0;32m    639\u001b[0m         )\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    852\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    853\u001b[0m         )\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_google_genai\\chat_models.py:949\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[1;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    925\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    937\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m    938\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[0;32m    939\u001b[0m         messages,\n\u001b[0;32m    940\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    947\u001b[0m         tool_choice\u001b[38;5;241m=\u001b[39mtool_choice,\n\u001b[0;32m    948\u001b[0m     )\n\u001b[1;32m--> 949\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m _chat_with_retry(\n\u001b[0;32m    950\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    952\u001b[0m         generation_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate_content,\n\u001b[0;32m    953\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata,\n\u001b[0;32m    954\u001b[0m     )\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_google_genai\\chat_models.py:196\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[1;34m(generation_method, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\tenacity\\__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\tenacity\\__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\tenacity\\__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[1;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\tenacity\\__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91630\\anaconda3\\envs\\Researchpaper\\lib\\site-packages\\langchain_google_genai\\chat_models.py:190\u001b[0m, in \u001b[0;36m_chat_with_retry.<locals>._chat_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[1;31mChatGoogleGenerativeAIError\u001b[0m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(r\"C:\\Projects\\Research_paper_classifier\\Research-Paper-Evaluater\\data\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "# Combine all document text\n",
    "all_text = \" \".join([doc.page_content for doc in documents])\n",
    "\n",
    "print(\"First 200 characters of the document text:\")\n",
    "print(all_text[:200])\n",
    "\n",
    "# Sanitize document\n",
    "sanitized_text = all_text.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "# Check if the document is non-empty\n",
    "if not sanitized_text.strip():\n",
    "    print(\"The document is empty or malformed.\")\n",
    "else:\n",
    "    inputs = {\"doc\": sanitized_text}\n",
    "    \n",
    "    final_output=report_decider(inputs)\n",
    "    print('final-ouput:',final_output)\n",
    "   \n",
    "    # Print the output of the Report_Generator node\n",
    "    print(f\"Report Generator Output: {final_output.get('Report_Generator')}\")\n",
    "\n",
    "    # Access the prediction from Report_Generator output\n",
    "    predictions = final_output.get('prediction', [])\n",
    "    print(f\"Predictions: {predictions}\")  # Debugging predictions\n",
    "\n",
    "    # If predictions are found, pass them to F1-calculator\n",
    "    if predictions:\n",
    "        inputs_for_f1 = {\"doc\": sanitized_text, \"prediction\": predictions}\n",
    "        f1_output=f1_score(inputs_for_f1)\n",
    "        print(f1_output)\n",
    "    else:\n",
    "        print(\"No predictions found from Report_Generator.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Researchpaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
