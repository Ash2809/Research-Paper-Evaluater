{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\areef\\anaconda3\\envs\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "import easyocr\n",
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_KEY_API=os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=GOOGLE_KEY_API\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model.invoke(f\"I will give you the Research Papers.Can you classify that it is publishible or not ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the research papers.  I need the text of the papers to assess their publishability.  My assessment will be based on several factors, including:\n",
      "\n",
      "* **Originality and significance:** Does the research address a novel problem or offer a significant contribution to the field?\n",
      "* **Methodology:** Is the research methodology sound, appropriate, and clearly described?  Are the data collection and analysis methods rigorous?\n",
      "* **Results:** Are the results clearly presented and well-supported by the data?  Are the findings meaningful and interpretable?\n",
      "* **Clarity and writing quality:** Is the paper well-written, easy to understand, and free of grammatical errors?  Is the structure logical and clear?\n",
      "* **Literature review:** Does the paper adequately review the relevant literature and position the research within the existing body of knowledge?\n",
      "* **Conclusions:** Are the conclusions justified by the results and appropriately discussed?\n",
      "* **Ethical considerations:**  Are there any ethical concerns related to the research design or conduct?\n",
      "\n",
      "Keep in mind that my assessment will be a preliminary evaluation.  The final decision on publishability rests with the journal editors and peer reviewers.  I can't guarantee the accuracy of my judgment, as I lack the expertise of a human reviewer in specific fields.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JueWu-MC: Achieving Sample-Efficient Minecraft\n",
      "Gameplay through Hierarchical Reinforcement\n",
      "Learning\n",
      "Abstract\n",
      "Learning rational behaviors in open-world games such as Minecraft continues to\n",
      "pose a challenge to Reinforcement Learning (RL) research, due to the combined\n",
      "difficulties of partial observability, high-dimensional visual perception, and delayed\n",
      "rewards. To overcome these challenges, we propose JueWu-MC, a sample-efficient\n",
      "hierarchical RL method that incorporates representation learning and imitation\n",
      "learning to handle perception and exploration. Our approach has two levels of\n",
      "hierarchy: the high-level controller learns a policy to manage options, while the\n",
      "low-level workers learn to solve each sub-task. To boost learning of sub-tasks,\n",
      "we propose a combination of techniques including: 1) action-aware represen-\n",
      "tation learning, which captures relations between action and representation; 2)\n",
      "discriminator-based self-imitation learning for efficient exploration; and 3) ensem-\n",
      "ble behavior cloning with consistency filtering for policy robustness. Extensive\n",
      "experiments demonstrate that JueWu-MC significantly enhances sample efficiency\n",
      "and outperforms several baselines. We won the championship of the MineRL 2021\n",
      "research competition and achieved the highest performance score.\n",
      "1\n",
      "Introduction\n",
      "Deep reinforcement learning (DRL) has achieved great success in numerous game genres including\n",
      "board games, Atari games, simple first-person-shooter (FPS) games, real-time strategy (RTS) games,\n",
      "and multiplayer online battle arena (MOBA) games. Recently, open-world games have garnered\n",
      "attention due to their playing mechanisms and their resemblance to real-world control tasks. Minecraft,\n",
      "as a typical open-world game, has been increasingly explored in recent years.\n",
      "Compared to other games, the properties of Minecraft make it an ideal testbed for RL research,\n",
      "because it emphasizes exploration, perception, and construction in a 3D open world. Agents are given\n",
      "partial observability and face occlusions. Tasks in the game are chained and long-term. Humans can\n",
      "typically make rational decisions to explore basic items and construct more complex items with a\n",
      "reasonable amount of practice, while it can be challenging for AI agents to do so autonomously. To\n",
      "facilitate the effective decision-making of agents in playing Minecraft, MineRL has been developed\n",
      "as a research competition platform, which provides human demonstrations and encourages the\n",
      "development of sample-efficient RL agents for playing Minecraft. Since its release, many efforts\n",
      "have been made to develop Minecraft AI agents.\n",
      "However, it remains difficult for current RL algorithms to acquire items in Minecraft due to several\n",
      "factors, which include the following. First, in order to reach goals, the agent is required to complete\n",
      "many sub-tasks that highly depend on each other. Due to the sparse reward, it is difficult for agents\n",
      "to learn long-horizon decisions efficiently. Hierarchical RL from demonstrations has been explored\n",
      "to take advantage of the task structure to accelerate learning. However, learning from unstructured\n",
      "demonstrations without any domain knowledge remains difficult. Second, Minecraft is a flexible\n",
      "3D first-person game which revolves around gathering resources and creating structures and items.\n",
      "In this environment, agents are required to handle high-dimensional visual input to enable efficient\n",
      ".\n",
      "control. However, the agent’s surroundings are varied and dynamic, which makes it difficult to learn\n",
      "a good representation. Third, with partial observability, the agent needs to explore in the right way\n",
      "and collect information from the environment in order to achieve goals. Naive exploration can waste\n",
      "a lot of samples on useless actions. Self-imitation learning (SIL) is a simple method that learns to\n",
      "reproduce past good behaviors to incentivize exploration, but it is not sample efficient. Lastly, human\n",
      "demonstrations are diverse and often noisy.\n",
      "To address these combined challenges, we propose an efficient hierarchical RL approach, equipped\n",
      "with novel representation and imitation learning techniques. Our method leverages human demonstra-\n",
      "tions to boost the learning of agents, enabling the RL algorithm to learn rational behaviors with high\n",
      "sample efficiency.\n",
      "Hierarchical Planning with Prior. We propose a hierarchical RL (HRL) framework with two\n",
      "levels of hierarchy. The high-level controller extracts sub-goals from human demonstrations and\n",
      "learns a policy to control options, while the low-level workers learn sub-tasks to achieve sub-goals\n",
      "by leveraging demonstrations and interactions with environments. Our approach structures the\n",
      "demonstrations and learns a hierarchical agent, which enables better decisions over long-horizon\n",
      "tasks. We use the following key techniques to boost agent learning.\n",
      "Action-aware Representation Learning. We propose action-aware representation learning (A2RL)\n",
      "to capture the relations between action and representation in 3D visual environments such as Minecraft.\n",
      "A2RL enables effective control and improves the interpretability of the learned policy.\n",
      "Discriminator-based Self-imitation Learning. We propose discriminator-based self-imitation\n",
      "learning (DSIL), which leverages self-generated experiences to learn self-correctable policies for\n",
      "better exploration.\n",
      "Ensemble Behavior Cloning with Consistency Filtering. We propose consistency filtering to\n",
      "identify common human behaviors, and then perform ensemble behavior cloning to learn a robust\n",
      "agent with reduced uncertainty.\n",
      "Our contributions are as follows: 1) We propose JueWu-MC, a sample-efficient hierarchical RL\n",
      "approach, equipped with action-aware representation learning, discriminator-based self-imitation,\n",
      "and ensemble behavior cloning with consistency filtering. 2) Our approach outperforms competitive\n",
      "baselines and achieves the best performance throughout the history of the competition.\n",
      "2\n",
      "Related Work\n",
      "2.1\n",
      "Game AI\n",
      "Games have long been a testing ground for artificial intelligence research. AlphaGo mastered the\n",
      "game of Go with DRL and tree search. Since then, DRL has been used in other sophisticated games,\n",
      "including StarCraft, Google Football, VizDoom, and Dota. Recently, the 3D open-world game\n",
      "Minecraft has been attracting attention. Previous research has shown that existing RL algorithms\n",
      "can struggle to generalize in Minecraft and a new memory-based DRL architecture was proposed to\n",
      "address this. Another approach combines a deep skill array and a skill distillation system to promote\n",
      "lifelong learning and transfer knowledge among different tasks. Since the MineRL competition began\n",
      "in 2019, many solutions have been proposed to learn to play in Minecraft. These works can be\n",
      "grouped into two categories: 1) end-to-end learning; 2) hierarchical RL with human demonstrations.\n",
      "Our approach belongs to the second category, which leverages the structure of the tasks and learns\n",
      "a hierarchical agent to play in Minecraft. ForgER proposed a hierarchical method with forgetful\n",
      "experience replay, and SEIHAI fully takes advantage of human demonstrations and task structure.\n",
      "2.2\n",
      "Sample-efficient Reinforcement Learning\n",
      "Our work aims to create a sample-efficient RL agent for playing Minecraft, and we thereby develop a\n",
      "combination of efficient learning techniques. We discuss the most relevant works below.\n",
      "Our work is related to HRL research that builds upon human priors. One approach proposes to\n",
      "warm-up the hierarchical agent from demonstrations and fine-tune it with RL algorithms. Another\n",
      "approach proposes to learn a skill prior from demonstrations to accelerate HRL algorithms. Compared\n",
      "to existing works, we are faced with highly unstructured demos in 3D first-person video games played\n",
      "2\n",
      "by the crowds. We address this challenge by structuring the demonstrations and defining sub-tasks\n",
      "and sub-goals automatically.\n",
      "Representation learning in RL has two broad directions: self-supervised learning and contrastive\n",
      "learning. Self-supervised learning aims to learn rich representations for high-dimensional unlabeled\n",
      "data to be useful across tasks. Contrastive learning learns representations that obey similarity\n",
      "constraints. Our work proposes a self-supervised representation learning method that measures action\n",
      "effects in 3D video games.\n",
      "Existing methods use curiosity or uncertainty as a signal for exploration so that the learned agent\n",
      "is able to cover a large state space. The exploration-exploitation dilemma drives us to develop self-\n",
      "imitation learning (SIL) methods that focus on exploiting past good experiences for better exploration.\n",
      "We propose discriminator-based self-imitation learning (DSIL).\n",
      "3\n",
      "Method\n",
      "In this section, we first introduce our overall HRL framework, and then describe each component in\n",
      "detail.\n",
      "3.1\n",
      "Overview\n",
      "Our overall framework is shown in Figure 1. We define human demonstrations as D = {τ0, τ1, τ2, ...}\n",
      "where τi is a long-horizon trajectory containing states, actions, and rewards. The provided demon-\n",
      "strations are unstructured, without explicit signals that specify sub-tasks and sub-goals.\n",
      "We define an atomic skill as a skill that gets a non-zero reward. We define sub-tasks and sub-goals\n",
      "based on the atomic skills. To define sub-tasks, we examine the reward delay for each atomic skill,\n",
      "keeping those with long reward delays as individual sub-tasks and merging those with short reward\n",
      "delays into one sub-task. Through this process, we have n sub-tasks in total. To define sub-goals for\n",
      "each sub-task, we extract the most common human behavior pattern and use the last state in each\n",
      "sub-task as its sub-goal. Through this, we have structured demonstrations (D →{D0, D1, ..., Dn−1}\n",
      ") with sub-tasks and sub-goals used to train the hierarchical agent. With the structured demonstrations,\n",
      "we train the meta-policy using imitation learning and train sub-policies to solve sub-tasks using\n",
      "demonstrations and interactions with the environment.\n",
      "3.2\n",
      "Meta- and Sub-policies\n",
      "Meta-policy. We train a meta-policy that maps continuous states to discrete indices (0, 1, ..., n - 1)\n",
      "that specify which option to use. Given state space S and discrete option o ∈O, the meta-policy is\n",
      "defined as πm(θ)(o|s), where s ∈S, o ∈O, and θ represents the parameters. πm(θ)(o|s) specifies\n",
      "the conditional distribution over the discrete options. To train the meta-policy, we generate training\n",
      "data (s, i) where i represents the i-th stage and s ∈Di is sampled from the demonstrations of the i-th\n",
      "stage. The meta-policy is trained using negative log-likelihood (NLL) loss:\n",
      "Lm = −Pn−1\n",
      "i=0 log πm(i|s)\n",
      "During inference, the meta-policy generates options by taking\n",
      "σ = argmaxoπm(o|s)\n",
      "Sub-policy. In Minecraft, sub-tasks can be grouped into two main types: gathering resources, and\n",
      "crafting items. In the first type (gathering resources), agents need to navigate and gather sparse\n",
      "rewards by observing high-dimensional visual inputs. In the second type (crafting items), agents need\n",
      "to execute a sequence of actions robustly.\n",
      "In typical HRL, the action space of the sub-policies is predefined. However, in the competition, a\n",
      "handcrafted action space is prohibited. Additionally, the action space is obfuscated in both human\n",
      "demonstrations and the environment. Learning directly in this continuous action space is challenging\n",
      "as exploration in a large continuous space can be inefficient. We use KMeans to cluster actions for\n",
      "each sub-task using demonstration Di, and perform reinforcement learning and imitation learning\n",
      "based on the clustered action space.\n",
      "3\n",
      "In the following section, we describe how to learn sub-policies efficiently to solve these two kinds of\n",
      "sub-tasks.\n",
      "3.3\n",
      "Learning Sub-policies to Gather Resources\n",
      "To efficiently solve these sub-tasks, we propose action-aware representation learning and\n",
      "discriminator-based self-imitation learning to facilitate the learning of sub-policies. The model\n",
      "architecture is shown in Figure 2.\n",
      "Action-aware Representation Learning. To learn compact representations, we observe that in 3D\n",
      "environments, different actions have different effects on high-dimensional observations. We propose\n",
      "action-aware representation learning (A2RL) to capture the relation with actions.\n",
      "We learn a mask net on a feature map for each action to capture dynamic information between the\n",
      "current and next states. Let the feature map be fθ(s) ∈RC×H×W and the mask net be mϕ(s, a) ∈\n",
      "[0, 1]H×W , where θ and ϕ represent parameters of the policy and mask net. Given a transition tuple\n",
      "(s, a, s′), the loss function for training the mask is:\n",
      "Lm(ϕ) = −Es,a,s′∼D[log(σ((fθ(s′) −gψ(fθ(s))) ⊙mϕ(s, a))) + η(1 −mϕ(s, a))]\n",
      "where gψ is a linear projection function parameterized by learnable parameters ψ; ⊙represents\n",
      "element-wise product, and η is a hyper-parameter that balances two objectives.\n",
      "To optimize the above loss function, we use a two-stage training process. In the first stage, we train\n",
      "the linear projection network gψa using the following objective:\n",
      "Lg(ψa) = Es,a,s′∼D[||fθ(s′) −gψa(fθ(s))||2]\n",
      "This objective learns to recover information of s′ from s in latent space, which is equal to learning a\n",
      "dynamic model to predict the next state given the current state and action. Note that the parameter ψ\n",
      "is dependent on the action a. In the second stage, we fix the learned linear function gψa and optimize\n",
      "the mask net.\n",
      "By minimizing the loss function, the mask net will learn to focus on local parts of the current image\n",
      "that are uncertain to the dynamic model. This is similar to human curiosity, which focuses on that\n",
      "which is uncertain.\n",
      "For policy-based methods, we integrate our learned representations into policy networks. For value-\n",
      "based methods, we combine our learned representations directly with Q-value functions. The learning\n",
      "of the Q-value function can be done using any Q-learning based algorithms.\n",
      "Discriminator-based Self-imitation Learning. We propose discriminator-based self-imitation\n",
      "learning (DSIL). Unlike ASIL, DSIL does not use advantage clipping. Our intuition is that the agent\n",
      "should be encouraged to visit the state distribution that is more likely to lead to goals.\n",
      "To do so, DSIL learns a discriminator to distinguish between states from successful and failed\n",
      "trajectories, and then uses the learned discriminator to guide exploration. We maintain two replay\n",
      "buffers B+\n",
      "i and B−\n",
      "i to store successful and failed trajectories. During learning, we treat data from\n",
      "B+\n",
      "i as positive samples and data from B−\n",
      "i as negative samples to train the discriminator. Let the\n",
      "discriminator be Dξ : S →[0, 1] which is parameterized by parameters ξ. We train the discriminator\n",
      "with the objective:\n",
      "maxξEs∈B+\n",
      "i [log Dξ(s)] + Es∈B−\n",
      "i [1 −log Dξ(s)]\n",
      "The discriminator is encouraged to output high values for good states and low values for bad states.\n",
      "For states that are not distinguishable, Dξ(s) tends to output 0.5.\n",
      "We use the trained discriminator to provide intrinsic rewards for policy learning to guide exploration.\n",
      "The intrinsic reward is defined as:\n",
      "r(s, a, s′) = { + 1ifDξ(s′) > 1 −ϵ\n",
      "−1ifDξ(s′) < ϵ\n",
      "where ϵ ∈(0, 0.5) is a hyper-parameter to control the confidence interval of Dξ. This reward drives\n",
      "the policy to explore in regions that previously led to successful trajectories. DSIL encourages the\n",
      "policy to stay close to a good state distribution, reproduce past decisions, and also be self-correctable.\n",
      "4\n",
      "3.4\n",
      "Learning Sub-policies to Craft Items\n",
      "In this type of sub-task, agents must learn a sequence of actions to craft items. To finish such tasks,\n",
      "agents need to learn a robust policy to execute a sequence of actions.\n",
      "We explore pure imitation learning (IL) to reduce the need for interactions with the environment, due\n",
      "to the limited sample and interaction usage. We propose ensemble behavior cloning with consistency\n",
      "filtering (EBC).\n",
      "Consistency Filtering. Human demonstrations can be diverse and noisy. Directly imitating such noisy\n",
      "data can cause confusion for the policy. Therefore, we perform consistency filtering by extracting\n",
      "the most common pattern of human behaviors. We extract the most common action sequence from\n",
      "demonstrations Di. For each trajectory, we keep those actions that lead to a state change while\n",
      "appearing for the first time to form an action sequence, and count the occurrences of each pattern.\n",
      "We then get the most common action pattern. Afterward, we conduct consistency filtering using the\n",
      "extracted action pattern.\n",
      "Ensemble Behavior Cloning. Learning policy from offline datasets can lead to generalization\n",
      "issues. Policies learned through behavior cloning may become uncertain when encountering unseen\n",
      "out-of-distribution states. To mitigate this, EBC learns a population of policies on different subsets of\n",
      "demonstrations to reduce the uncertainty of the agent’s decision. Specifically, we train K policies on\n",
      "different demonstrations with NLL loss:\n",
      "minθkEs,a∼¯\n",
      "Dk\n",
      "i [−log πθk(a|s)], ¯\n",
      "Dk\n",
      "i ⊂¯\n",
      "Di, k = 1, 2, ..., K\n",
      "where θk parameterizes the k-th policy. During inference, EBC adopts a majority voting mechanism\n",
      "to select an action that is the most confident among the policies.\n",
      "4\n",
      "Experiment\n",
      "We conduct experiments using the MineRL environment. Our approach is built based on RL\n",
      "algorithms including SQIL, PPO, and DQfD.\n",
      "4.1\n",
      "Main Results\n",
      "Table 1 shows all the MineRL competition results since 2019. The competition settings in 2020 and\n",
      "2021 were more difficult than in 2019. In these years, participants had to focus on the algorithm\n",
      "design itself. The scores in 2020 and 2021 are lower than in 2019. Our approach outperforms all\n",
      "previous solutions. End-to-end baselines cannot achieve a decent result, showing it is difficult to\n",
      "solve long-horizon tasks with end-to-end learning. Compared to the results of the 2020 competition,\n",
      "our method outperforms other solutions with a score (76.97) that is 3.4x higher than the second place\n",
      "score (22.97). Table 2 shows the conditional success rate of each stage between our approach and\n",
      "SEIHAI. Our approach outperforms SEIHAI in every stage.\n",
      "Figure 3(a) shows the training curves. Due to a version update of MineRL 2021, our online score\n",
      "dropped compared with the performance in our training curve. Our approach is sample-efficient and\n",
      "outperforms prior best results with 0.5 million training samples. Our score reaches 100 with 2.5\n",
      "million training samples, which is less than the 8 million samples of the MineRL competition.\n",
      "4.2\n",
      "Ablation Study\n",
      "To examine the effectiveness of our proposed techniques, we consider three variants of our approach:\n",
      "1) without A2RL, 2) without DSIL, and 3) without EBC. Figure 3(b) shows the training curves. Each\n",
      "technique contributes to the overall performance. EBC and A2RL contribute more than DSIL. DSIL\n",
      "mainly boosts the performance for later sub-tasks, while A2RL and EBC have earlier effects on\n",
      "the overall pipeline. EBC contributes significantly, demonstrating that learning a robust policy is\n",
      "important for solving long-horizon tasks.\n",
      "5\n",
      "4.3\n",
      "Visualization\n",
      "To understand why our techniques work, we conduct an in-depth analysis. To understand the learned\n",
      "mask in A2RL, we compute saliency maps. For each action, we show the current state, the next state,\n",
      "and the saliency map of the learned mask on the current state. We find that the learned mask captures\n",
      "the dynamic information between two adjacent states, revealing curiosity on the effect of actions. The\n",
      "mask net learns to focus on uncertain parts of the current state. For the ’attack’ action, the learned\n",
      "mask focuses on the objects in front of the agent. For the ’turn left’ and ’turn down’ actions, the mask\n",
      "net focuses on the parts that have major changes due to the rotation and translation of the agent’s\n",
      "perspective. Our learned mask assists the agent in better understanding the 3D environment.\n",
      "To understand how DSIL works, we visualize the state distribution that the agent visits. We compare\n",
      "PPO, PPO+SIL, and PPO+DSIL. At the early training stage, both methods explore randomly and\n",
      "sometimes reach the goal state successfully. After getting samples and training, PPO+DSIL starts to\n",
      "explore in a compact region, while PPO and PPO+SIL still explore in a wider region. DSIL pushes\n",
      "the agent to stay close to a good state distribution, reproducing its past behaviors and exploring in a\n",
      "better way, which incentivizes deep exploration for successful trajectories.\n",
      "Table 1: MineRL Competition Results. Our solution (JueWu-MC) significantly outperforms all other\n",
      "competitive solutions.\n",
      "Baselines\n",
      "2019 Competition Results\n",
      "Name\n",
      "Score\n",
      "Team Name\n",
      "Score\n",
      "SQIL\n",
      "2.94\n",
      "CDS (ForgER)\n",
      "61.61\n",
      "DQfD\n",
      "2.39\n",
      "mcrl\n",
      "42.41\n",
      "Rainbow\n",
      "0.42\n",
      "I4DS\n",
      "40.8\n",
      "PDDDQN\n",
      "0.11\n",
      "CraftRL\n",
      "23.81\n",
      "BC\n",
      "2.40\n",
      "UEFDRL\n",
      "17.9\n",
      "TD240\n",
      "15.19\n",
      "2020 Competition Results\n",
      "2021 Competition Results\n",
      "Team Name\n",
      "Score\n",
      "Team Name\n",
      "Score\n",
      "HelloWorld (SEIHAI)\n",
      "39.55\n",
      "X3 (JueWu-MC)\n",
      "76.97\n",
      "michal_opanowicz\n",
      "13.29\n",
      "WinOrGoHome\n",
      "22.97\n",
      "NoActionWasted\n",
      "12.79\n",
      "MCAgent\n",
      "18.98\n",
      "Rabbits\n",
      "5.16\n",
      "sneakysquids\n",
      "14.35\n",
      "MajiManji\n",
      "2.49\n",
      "JBR_HSE\n",
      "10.33\n",
      "BeepBoop\n",
      "1.97\n",
      "zhongguodui\n",
      "8.84\n",
      "Table 2: The conditional success rate of each stage.\n",
      "Methods\n",
      "Stage 1\n",
      "Stage 2\n",
      "Stage 3\n",
      "Stage 4\n",
      "Stage 5\n",
      "Stage 6\n",
      "Stage 7\n",
      "SEIHAI\n",
      "64%\n",
      "78.6%\n",
      "78.3%\n",
      "84.7%\n",
      "23%\n",
      "0%\n",
      "0%\n",
      "JueWu-MC\n",
      "92%\n",
      "96%\n",
      "96%\n",
      "87%\n",
      "46%\n",
      "11%\n",
      "0%\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we present JueWu-MC, a sample-efficient hierarchical reinforcement learning frame-\n",
      "work designed to play Minecraft. With a high-level controller and several auto-extracted low-level\n",
      "workers, our framework can adapt to different environments and solve sophisticated tasks. Our\n",
      "novel techniques in representation learning and imitation learning improve both the performance and\n",
      "learning efficiency of the sub-policies. Experiments show that our pipeline outperforms all baseline\n",
      "algorithms and previous solutions from MineRL competitions. In future work, we would like to apply\n",
      "JueWu-MC to other Minecraft tasks, as well as other open-world games.\n",
      "6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz \n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    full_text = \"\"\n",
    "    \n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        \n",
    "        page_text = page.get_text()\n",
    "        \n",
    "        full_text += page_text\n",
    "    \n",
    "    return full_text\n",
    "pdf_path = r'C:\\Users\\areef\\Desktop\\Gen AI\\P095.pdf'  \n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_KEY_API = os.getenv(\"GOOGLE_API_KEY\")\n",
    "model = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    convert_system_message_to_human=True,\n",
    "    google_api_key=GOOGLE_KEY_API\n",
    ")\n",
    "\n",
    "def classify_paper(text):\n",
    "    prompt=\"\"\"You are an expert evaluator tasked with classifying a research paper as 'Publishable' (1) or 'Not Publishable' (0). \n",
    "        Content of the research paper:  \n",
    "        {text}\n",
    "        classifying a research paper as 'Publishable' (1) or 'Not Publishable' (0)\"\"\"\n",
    "    response = model.invoke(prompt)\n",
    "    \n",
    "    if 'Publishable' in response:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "def process_text_and_classify(pdf_path, extracted_text):\n",
    "    paper_name = os.path.basename(pdf_path)  \n",
    "    results = [] \n",
    "    \n",
    "    label = classify_paper(extracted_text)\n",
    "    \n",
    "    results.append([paper_name, pdf_path, label])\n",
    "\n",
    "    \n",
    "    if results:  \n",
    "        with open('classified_paper.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Paper Name\", \"Path\", \"Label\"])  \n",
    "            writer.writerows(results)  \n",
    "    else:\n",
    "        print(\"No paper was processed.\")\n",
    "\n",
    "# pdf_path = r'C:\\Users\\areef\\Desktop\\Gen AI\\R001.pdf' \n",
    "xyz = extracted_text  \n",
    "process_text_and_classify(pdf_path, xyz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
